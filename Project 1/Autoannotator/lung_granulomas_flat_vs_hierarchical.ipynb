{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9637630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras as ks\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c233e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_final = \"../Data/controls_final.h5ad\"\n",
    "granulomas_final = \"../Data/granulomas_final.h5ad\"\n",
    "sc78_final = \"../Data/sc78_final.h5ad\"\n",
    "sc92_final = \"../Data/sc92_final.h5ad\"\n",
    "sc93_final = \"../Data/sc93_final.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls_final_anndata = sc.read_h5ad(controls_final) \n",
    "# granulomas_final_anndata = sc.read_h5ad(granulomas_final)\n",
    "sc78_final_anndata = sc.read_h5ad(sc78_final)\n",
    "sc92_final_anndata = sc.read_h5ad(sc92_final)\n",
    "sc93_final_anndata = sc.read_h5ad(sc93_final)\n",
    "\n",
    "def fix_and_print(anndata):\n",
    "    anndata.uns['log1p']['base'] = None\n",
    "    print(anndata.obs['sample'].value_counts())\n",
    "    print('X matrix is sparse:', scipy.sparse.issparse(anndata.X))\n",
    "    print('X size =', anndata.X.shape)\n",
    "    print()\n",
    "\n",
    "# fix_and_print(controls_final_anndata)\n",
    "# fix_and_print(granulomas_final_anndata)\n",
    "fix_and_print(sc78_final_anndata)\n",
    "fix_and_print(sc92_final_anndata)\n",
    "fix_and_print(sc93_final_anndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_final_annotation_dict = {\n",
    "    '0': 'CAP1',\n",
    "    '12': 'CAP2',\n",
    "    '21': 'VEC',\n",
    "    '17': 'AEC',\n",
    "    '28': 'LEC',\n",
    "    '14': 'Ciliated',\n",
    "    '5': 'Secretory',\n",
    "    '1': 'AT1',\n",
    "    '2': 'AT2',\n",
    "    '3': 'AF',\n",
    "    '20': 'Pericyte',\n",
    "    '26': 'SMC',\n",
    "    '18': 'Mesothelial',\n",
    "    '8': 'B1',\n",
    "    '23b': 'Th1',\n",
    "    '11': 'Tnaive',\n",
    "    '24': 'NK',\n",
    "    '10': 'AM',\n",
    "    '15b': 'M-C1q',\n",
    "    '25': 'iMon',\n",
    "    '15': 'DC',\n",
    "    '15c': 'pDC',\n",
    "    '22': 'N1',\n",
    "}\n",
    "\n",
    "granulomas_final_annotation_dict = {\n",
    "    '9': 'CAP1',\n",
    "    '24': 'CAP2',\n",
    "    '9b': 'VEC',\n",
    "    '27': 'LEC',\n",
    "    '17': 'Ciliated',\n",
    "    '15': 'Secretory',\n",
    "    '22': 'AT1',\n",
    "    '6': 'AT2',\n",
    "    '12': 'AT2-t1',\n",
    "    '19': 'AT2-t2',\n",
    "    '14': 'AF',\n",
    "    '25': 'Pericyte',\n",
    "    '20': 'Mesothelial',\n",
    "    '3': 'B1',\n",
    "    '3b': 'B2',\n",
    "    '0': 'Th1',\n",
    "    '8': 'Tnaive',\n",
    "    '11': 'Tex',\n",
    "    '77': 'Treg',\n",
    "    '11b': 'NK',\n",
    "    '4a': 'AM',\n",
    "    '4': 'M-t1',\n",
    "    '10': 'M-lc',\n",
    "    '7': 'M-t2',\n",
    "    '7b': 'M-C1q',\n",
    "    '7c': 'iMon',\n",
    "    '23': 'pDC',\n",
    "    '13': 'DC',\n",
    "    '5b': 'N1',\n",
    "    '5': 'N2',\n",
    "}\n",
    "\n",
    "sc78_final_annotation_dict = {\n",
    "    '3': 'CAP1',\n",
    "    '3b': 'CAP2',\n",
    "    '3c': 'VEC',\n",
    "    '25': 'LEC',\n",
    "    '16': 'Ciliated',\n",
    "    '8': 'Secretory',\n",
    "    '17': 'AT1',\n",
    "    '7': 'AT2',\n",
    "    '10': 'AT2-t1',\n",
    "    '9': 'AT2-t2',\n",
    "    '15': 'AF',\n",
    "    '23': 'Pericyte',\n",
    "    '21': 'Mesothelial',\n",
    "    '13': 'B1',\n",
    "    'Th1': 'Th1',\n",
    "    'Tnaive': 'Tnaive',\n",
    "    'Treg': 'Treg',\n",
    "    'Tex': 'Tex',\n",
    "    'NK': 'NK',\n",
    "    '11': 'AM',\n",
    "    '2b': 'M-C1q',\n",
    "    '0': 'M-t1',\n",
    "    '2': 'M-t2',\n",
    "    '2c': 'iMon', \n",
    "    '18': 'DC',\n",
    "    '18b': 'pDC',\n",
    "    '22': 'N1',\n",
    "    '5': 'N2'\n",
    "}\n",
    "\n",
    "sc92_final_annotation_dict = {\n",
    "    '11': 'CAP1',\n",
    "    '24': 'CAP2',\n",
    "    '15': 'VEC',\n",
    "    '28': 'LEC', \n",
    "    '18': 'Ciliated',\n",
    "    '14': 'Secretory',\n",
    "    '10': 'AT1',\n",
    "    '4': 'AT2',\n",
    "    '12': 'AT2-t1', \n",
    "    '8': 'AF',\n",
    "    '25': 'Pericyte',\n",
    "    '8c': 'SMC',\n",
    "    '21b': 'Mesothelial',\n",
    "    '6': 'B1',\n",
    "    '26': 'B2',\n",
    "    'Th1': 'Th1',\n",
    "    'Treg': 'Treg',\n",
    "    'Tex': 'Tex',\n",
    "    'Tnaive': 'Tnaive',\n",
    "    'Tnaive': 'Tnaive',\n",
    "    'T': 'T',  \n",
    "    '13': 'AM',\n",
    "    '1': 'M-t1',\n",
    "    '2': 'M-t2',\n",
    "    '2b': 'M-C1q',\n",
    "    '19b': 'pDC',\n",
    "    '19': 'DC',\n",
    "    '0': 'N2'\n",
    "}\n",
    "\n",
    "sc93_final_annotation_dict = {\n",
    "    '9': 'CAP1',\n",
    "    '9b': 'CAP2',\n",
    "    '9c': 'VEC',\n",
    "    '25': 'LEC',  \n",
    "    '10': 'Ciliated',\n",
    "    '13': 'Secretory',\n",
    "    '11': 'AT1',\n",
    "    '7': 'AT2',\n",
    "    '16': 'AT2-t1',\n",
    "    '3': 'AF',\n",
    "    '28': 'Pericyte',\n",
    "    '19': 'Mesothelial',\n",
    "    '0': 'B1',\n",
    "    '22': 'B2',\n",
    "    '4': 'Th1',\n",
    "    '4b': 'Treg',\n",
    "    '2': 'Tnaive',\n",
    "    '14': 'Tex',\n",
    "    '14b': 'NK',\n",
    "    '15': 'AM',\n",
    "    '1': 'M-t1',\n",
    "    '12': 'M-t2',\n",
    "    '12b': 'M-C1q',\n",
    "    '29': 'pDC',\n",
    "    '21': 'DC',\n",
    "    '26': 'N2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b077391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation(anndata, annotation_dict):\n",
    "    anndata.obs['single_cell_types'] = [annotation_dict[clust] for clust in anndata.obs['my_clust_1']]\n",
    "    dict_list = list(annotation_dict.keys())\n",
    "    anndata_list = list(anndata.obs['my_clust_1'].unique())\n",
    "    print('Keys in dictionary not in anndata:', [item for item in dict_list if item not in anndata_list])\n",
    "    print('Keys in anndata not in dictionary:', [item for item in anndata_list if item not in dict_list])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_annotation(controls_final_anndata, controls_final_annotation_dict)\n",
    "# create_annotation(granulomas_final_anndata, granulomas_final_annotation_dict)\n",
    "create_annotation(sc78_final_anndata, sc78_final_annotation_dict)\n",
    "create_annotation(sc92_final_anndata, sc92_final_annotation_dict)\n",
    "create_annotation(sc93_final_anndata, sc93_final_annotation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(anndata):\n",
    "    unique_celltype_sub = anndata.obs['single_cell_types'].unique()\n",
    "    print(unique_celltype_sub)\n",
    "    num_unique_celltype_sub = anndata.obs['single_cell_types'].nunique()\n",
    "    print(f\"Number of unique sub cell types: {num_unique_celltype_sub}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58484615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_stats(controls_final_anndata)\n",
    "# print_stats(granulomas_final_anndata)\n",
    "print_stats(sc78_final_anndata)\n",
    "print_stats(sc92_final_anndata)\n",
    "print_stats(sc93_final_anndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d37d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_subset(name, anndata, split, seed):\n",
    "    train_anndata, test_anndata = train_test_split(anndata.obs.index, test_size=split, random_state=seed, stratify=anndata.obs['single_cell_types'].values)\n",
    "    print(f'{name}_train_anndata shape', train_anndata.shape)\n",
    "    print(f'{name}_test_anndata shape', test_anndata.shape)\n",
    "    print()\n",
    "\n",
    "    train_anndata = anndata[train_anndata].copy()\n",
    "    test_anndata = anndata[test_anndata].copy()\n",
    "\n",
    "    train_anndata.write(f\"../Data/{name}_train_anndata.h5ad\")\n",
    "    test_anndata.write(f\"../Data/{name}_test_anndata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec04db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 8653\n",
    "split = 0.2\n",
    "# holdout_subset(\"controls_final\", controls_final_anndata, split, seed)\n",
    "# holdout_subset(\"granulomas_final\", granulomas_final_anndata, split, seed)\n",
    "holdout_subset(\"sc78_final\", sc78_final_anndata, split, seed)\n",
    "holdout_subset(\"sc92_final\", sc92_final_anndata, split, seed)\n",
    "holdout_subset(\"sc93_final\", sc93_final_anndata, split, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls_final_anndata = sc.read_h5ad(\"../Data/controls_final_train_anndata.h5ad\") \n",
    "granulomas_final_anndata = sc.read_h5ad(\"../Data/granulomas_final_train_anndata.h5ad\")\n",
    "sc78_final_anndata = sc.read_h5ad(\"../Data/sc78_final_train_anndata.h5ad\")\n",
    "sc92_final_anndata = sc.read_h5ad(\"../Data/sc92_final_train_anndata.h5ad\")\n",
    "sc93_final_anndata = sc.read_h5ad(\"../Data/sc93_final_train_anndata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_look(anndata):\n",
    "    anndata_hvg = anndata[:, anndata.var['highly_variable']].copy()\n",
    "    print(anndata_hvg)\n",
    "    print(anndata_hvg.obs['sample'].value_counts())\n",
    "    print(list(anndata_hvg.obs['single_cell_types'].unique()))\n",
    "    print(anndata_hvg.obs['single_cell_types'].nunique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_look(controls_final_anndata)\n",
    "quick_look(granulomas_final_anndata)\n",
    "quick_look(sc78_final_anndata)\n",
    "quick_look(sc92_final_anndata)\n",
    "quick_look(sc93_final_anndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dict = {\n",
    "    'Endothelial': ['CAP1','CAP2','VEC','AEC','LEC'],\n",
    "    'Epithelial': ['Ciliated','Secretory','AT1','AT2','AT2-t1','AT2-t2'],\n",
    "    'Mesenchyme': ['AF','Pericyte','SMC','Mesothelial'],\n",
    "    'Immune': ['B1','B2','Th1','Tnaive','Treg','Tex','T','NK','AM','M-t1','M-t2','M-C1q','M-lc','iMon','DC','pDC','N1','N2'] # added 'T' b/c of sc92\n",
    "}\n",
    "\n",
    "second_dict = {\n",
    "    'Blood vessels': ['CAP1','CAP2','VEC','AEC'],\n",
    "    'Lymphatic EC': ['LEC'],\n",
    "    'Airway epithelium': ['Ciliated','Secretory'],\n",
    "    'Alveolar epithelium' : ['AT1','AT2','AT2-t1','AT2-t2'],\n",
    "    'Stromal': ['AF','Pericyte','SMC'],\n",
    "    'Mesothelial': ['Mesothelial'],\n",
    "    'Lymphoid': ['B1','B2','Th1','Tnaive','Treg','Tex','T','NK'], # added 'T' b/c of sc92\n",
    "    'Myeloid': ['AM','M-t1','M-t2','M-C1q','M-lc','iMon','DC','pDC','N1','N2']\n",
    "}\n",
    "\n",
    "third_dict = {\n",
    "    'Blood vessels': ['CAP1','CAP2','VEC','AEC'],\n",
    "    'Lymphatic EC': ['LEC'],\n",
    "    'Airway epithelium': ['Ciliated','Secretory'],\n",
    "    'Alveolar epithelium': ['AT1','AT2','AT2-t1','AT2-t2'],\n",
    "    'Fibroblast': ['AF','Pericyte'],\n",
    "    'Smooth muscle': ['SMC'],\n",
    "    'Mesothelial': ['Mesothelial'],\n",
    "    'B lineage': ['B1','B2'],\n",
    "    'T lineage': ['Th1','Tnaive','Treg','Tex','T'], # added 'T' b/c of sc92\n",
    "    'NK': ['NK'],\n",
    "    'mononuclear broad': ['AM','M-t1','M-t2','M-C1q','M-lc','iMon','DC','pDC'], # originally two mononuclear\n",
    "    'Neutrophil': ['N1','N2'] # originally polymorphonuclear=Neutrophil\n",
    "}\n",
    "\n",
    "fourth_dict = {\n",
    "    'Blood vessels': ['CAP1','CAP2','VEC','AEC'],\n",
    "    'Lymphatic EC': ['LEC'],\n",
    "    'Airway epithelium': ['Ciliated','Secretory'],\n",
    "    'Alveolar epithelium': ['AT1','AT2','AT2-t1','AT2-t2'],\n",
    "    'Fibroblast': ['AF','Pericyte'],\n",
    "    'Smooth muscle': ['SMC'],\n",
    "    'Mesothelial': ['Mesothelial'],\n",
    "    'B lineage': ['B1','B2'],\n",
    "    'T lineage': ['Th1','Tnaive','Treg','Tex','T'], # added 'T' b/c of sc92\n",
    "    'NK': ['NK'],\n",
    "    'Macrophage': ['AM','M-t1','M-t2','M-C1q','M-lc'],        \n",
    "    'mononuclear fine': ['iMon','DC','pDC'], # originally two mononuclear\n",
    "    'Neutrophil': ['N1','N2']\n",
    "}\n",
    "\n",
    "L1_annotation = {}\n",
    "for cell_type, cluster_num in top_dict.items():\n",
    "    for x in cluster_num:\n",
    "        L1_annotation[x] = cell_type\n",
    "\n",
    "L2_annotation = {}\n",
    "for cell_type, cluster_num in second_dict.items():\n",
    "    for x in cluster_num:\n",
    "        L2_annotation[x] = cell_type\n",
    "\n",
    "L3_annotation = {}\n",
    "for cell_type, cluster_num in third_dict.items():\n",
    "    for x in cluster_num:\n",
    "        L3_annotation[x] = cell_type\n",
    "\n",
    "L4_annotation = {}\n",
    "for cell_type, cluster_num in fourth_dict.items():\n",
    "    for x in cluster_num:\n",
    "        L4_annotation[x] = cell_type\n",
    "\n",
    "def create_hierarchical_annotations(anndata):\n",
    "    anndata.obs[\"top_level\"] = anndata.obs['single_cell_types'].map(L1_annotation)\n",
    "    anndata.obs[\"second_level\"] = anndata.obs['single_cell_types'].map(L2_annotation)\n",
    "    anndata.obs[\"third_level\"] = anndata.obs['single_cell_types'].map(L3_annotation)\n",
    "    anndata.obs[\"fourth_level\"] = anndata.obs['single_cell_types'].map(L4_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aea889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_hierarchical_annotations(controls_final_anndata)\n",
    "# create_hierarchical_annotations(granulomas_final_anndata)\n",
    "create_hierarchical_annotations(sc78_final_anndata)\n",
    "create_hierarchical_annotations(sc92_final_anndata)\n",
    "create_hierarchical_annotations(sc93_final_anndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a267786",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = ['top_level', 'second_level', 'third_level', 'fourth_level', 'single_cell_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e450aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls_final_hierarchy_dict = {}\n",
    "# granulomas_final_hierarchy_dict = {}\n",
    "sc78_final_hierarchy_dict = {}\n",
    "sc92_final_hierarchy_dict = {}\n",
    "sc93_final_hierarchy_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a7a7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(root, path):\n",
    "    node = root\n",
    "    prev = None \n",
    "    for label in path:\n",
    "        if (label == prev): continue\n",
    "        node = node.setdefault(label, {})\n",
    "        prev = label\n",
    "\n",
    "def create_hierarchy_dict(anndata, hierarchy_dict):\n",
    "    unique_paths = anndata.obs[hierarchy].drop_duplicates().values\n",
    "    for path in unique_paths: add_path(hierarchy_dict, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b36b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_hierarchy_dict(controls_final_anndata, controls_final_hierarchy_dict)\n",
    "# create_hierarchy_dict(granulomas_final_anndata, granulomas_final_hierarchy_dict)\n",
    "create_hierarchy_dict(sc78_final_anndata, sc78_final_hierarchy_dict)\n",
    "create_hierarchy_dict(sc92_final_anndata, sc92_final_hierarchy_dict)\n",
    "create_hierarchy_dict(sc93_final_anndata, sc93_final_hierarchy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b323fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hierarchy_dict(name, hierarchy_dict):\n",
    "    with open(f\"../Data/{name}_hierarchy_dict.json\", \"w\") as file:\n",
    "        json.dump(hierarchy_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830add16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_hierarchy_dict(\"controls_final\", controls_final_hierarchy_dict)\n",
    "# save_hierarchy_dict(\"granulomas_final\", granulomas_final_hierarchy_dict)\n",
    "save_hierarchy_dict(\"sc78_final\", sc78_final_hierarchy_dict)\n",
    "save_hierarchy_dict(\"sc92_final\", sc92_final_hierarchy_dict)\n",
    "save_hierarchy_dict(\"sc93_final\", sc93_final_hierarchy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82243470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name(input): return re.sub(r\"[^A-Za-z0-9]+\", \"_\", input).strip(\"_\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c059391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_leaves(tree):\n",
    "    res = []\n",
    "    for key, value in tree.items():\n",
    "        if value: res.extend(get_leaves(value))\n",
    "        else: res.append(key)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db8d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_node(cell_name, dataset_name, node, anndata, split, seed):\n",
    "    sub_dict = {key: get_leaves(value) if value else [key] for key, value in node.items()}\n",
    "    int_mapping = {key: idx for idx, key in enumerate(sub_dict)}\n",
    "\n",
    "    cell_name = create_name(cell_name)\n",
    "    with open(f\"../Data/{dataset_name}_int_mapping_{cell_name}.json\", \"w\") as file:\n",
    "        json.dump(int_mapping, file, indent=4)\n",
    "\n",
    "    reverse_mapping = {value: key for key, values in sub_dict.items() for value in values}\n",
    "    \n",
    "    finest_level = hierarchy[-1]\n",
    "    anndata_subset = anndata[anndata.obs[finest_level].isin(reverse_mapping)].copy()\n",
    "    anndata_subset.obs[\"cell_names\"] = anndata_subset.obs[finest_level].map(reverse_mapping)\n",
    "    anndata_subset.obs[\"cell_integers\"] = anndata_subset.obs[\"cell_names\"].map(int_mapping)\n",
    "    anndata_subset_hvg = anndata_subset[:, anndata_subset.var['highly_variable']].copy()\n",
    "\n",
    "    if scipy.sparse.issparse(anndata_subset_hvg.X):\n",
    "        X = anndata_subset_hvg.X.toarray()\n",
    "    else:\n",
    "        X = anndata_subset_hvg.X\n",
    "    \n",
    "    y = anndata_subset_hvg.obs[\"cell_integers\"].values\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(X, y, test_size=split, random_state=seed, stratify=y)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "    train_features = np.array(train_features)\n",
    "    val_features = np.array(val_features)\n",
    "    train_labels = np.array(train_labels)\n",
    "    val_labels = np.array(val_labels)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    np.save(f'../Arrays/{dataset_name}_train_features_hvg_{cell_name}.npy', train_features)\n",
    "    np.save(f'../Arrays/{dataset_name}_val_features_hvg_{cell_name}.npy', val_features)\n",
    "    np.save(f'../Arrays/{dataset_name}_train_labels_hvg_{cell_name}.npy', train_labels)\n",
    "    np.save(f'../Arrays/{dataset_name}_val_labels_hvg_{cell_name}.npy', val_labels)\n",
    "    np.save(f'../Arrays/{dataset_name}_weights_hvg_{cell_name}.npy', weights)\n",
    "\n",
    "    anndata_subset_hvg.write(f\"../Data/{dataset_name}_train_anndata_hvg_{cell_name}.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_leaf(cell_name, dataset_name, leaf, anndata, split, seed):\n",
    "    int_mapping = {key: idx for idx, key in enumerate(leaf)}\n",
    "\n",
    "    cell_name = create_name(cell_name)\n",
    "    with open(f\"../Data/{dataset_name}_int_mapping_{cell_name}.json\", \"w\") as file:\n",
    "        json.dump(int_mapping, file, indent=4)\n",
    "\n",
    "    finest_level = hierarchy[-1]\n",
    "    anndata_subset = anndata[anndata.obs[finest_level].isin(leaf)].copy()\n",
    "    anndata_subset.obs[\"cell_integers\"] = anndata_subset.obs[finest_level].map(int_mapping)\n",
    "    anndata_subset_hvg = anndata_subset[:, anndata_subset.var['highly_variable']].copy()\n",
    "\n",
    "    if scipy.sparse.issparse(anndata_subset_hvg.X):\n",
    "        X = anndata_subset_hvg.X.toarray()\n",
    "    else:\n",
    "        X = anndata_subset_hvg.X\n",
    "    \n",
    "    y = anndata_subset_hvg.obs[\"cell_integers\"].values\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(X, y, test_size=split, random_state=seed, stratify=y)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "    train_features = np.array(train_features)\n",
    "    val_features = np.array(val_features)\n",
    "    train_labels = np.array(train_labels)\n",
    "    val_labels = np.array(val_labels)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    np.save(f'../Arrays/{dataset_name}_train_features_hvg_{cell_name}.npy', train_features)\n",
    "    np.save(f'../Arrays/{dataset_name}_val_features_hvg_{cell_name}.npy', val_features)\n",
    "    np.save(f'../Arrays/{dataset_name}_train_labels_hvg_{cell_name}.npy', train_labels)\n",
    "    np.save(f'../Arrays/{dataset_name}_val_labels_hvg_{cell_name}.npy', val_labels)\n",
    "    np.save(f'../Arrays/{dataset_name}_weights_hvg_{cell_name}.npy', weights)\n",
    "\n",
    "    anndata_subset_hvg.write(f\"../Data/{dataset_name}_train_anndata_hvg_{cell_name}.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce49ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_classification(cell_name, dataset_name, dict, anndata, split, seed):\n",
    "    children = list(dict)\n",
    "    if (not children): return\n",
    "\n",
    "    if all(not dict[child] for child in children):\n",
    "        preprocess_leaf(cell_name, dataset_name, children, anndata, split, seed)\n",
    "        return\n",
    "    \n",
    "    preprocess_node(cell_name, dataset_name, dict, anndata, split, seed)\n",
    "    for child in children: hierarchical_classification(child, dataset_name, dict[child], anndata, split, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6296\n",
    "split = 0.2\n",
    "# hierarchical_classification(\"top_level\", \"controls_final\", controls_final_hierarchy_dict, controls_final_anndata, split, seed)\n",
    "# hierarchical_classification(\"top_level\", \"granulomas_final\", granulomas_final_hierarchy_dict, granulomas_final_anndata, split, seed)\n",
    "hierarchical_classification(\"top_level\", \"sc78_final\", sc78_final_hierarchy_dict, sc78_final_anndata, split, seed)\n",
    "hierarchical_classification(\"top_level\", \"sc92_final\", sc92_final_hierarchy_dict, sc92_final_anndata, split, seed)\n",
    "hierarchical_classification(\"top_level\", \"sc93_final\", sc93_final_hierarchy_dict, sc93_final_anndata, split, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c1ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data to confirm\n",
    "\n",
    "cell_name = create_name(\"lymphoid\")\n",
    "dataset_name = \"sc93_final\"\n",
    "\n",
    "anndata = sc.read_h5ad(f\"../Data/{dataset_name}_train_anndata_hvg_{cell_name}.h5ad\")\n",
    "\n",
    "print(anndata)\n",
    "print()\n",
    "print(anndata.obs[\"single_cell_types\"].value_counts())\n",
    "print(anndata.obs[\"single_cell_types\"].unique())\n",
    "print()\n",
    "# print(anndata.obs[\"cell_names\"].value_counts())\n",
    "# print(anndata.obs[\"cell_names\"].unique())\n",
    "# print()\n",
    "print(anndata.obs[\"cell_integers\"].value_counts())\n",
    "print(anndata.obs[\"cell_integers\"].unique())\n",
    "print()\n",
    "\n",
    "train_features = np.load(f\"../Arrays/{dataset_name}_train_features_hvg_{cell_name}.npy\")\n",
    "val_features = np.load(f\"../Arrays/{dataset_name}_val_features_hvg_{cell_name}.npy\")\n",
    "train_labels = np.load(f\"../Arrays/{dataset_name}_train_labels_hvg_{cell_name}.npy\")\n",
    "val_labels = np.load(f\"../Arrays/{dataset_name}_val_labels_hvg_{cell_name}.npy\")\n",
    "weights = np.load(f\"../Arrays/{dataset_name}_weights_hvg_{cell_name}.npy\")\n",
    "\n",
    "print('train features shape:', train_features.shape)\n",
    "print('val features shape:', val_features.shape)\n",
    "print('train labels shape:', train_labels.shape)\n",
    "print('val labels shape:', val_labels.shape)\n",
    "print('weights shape:', weights.shape)\n",
    "print()\n",
    "\n",
    "class_weights = dict(enumerate(weights))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_classification(dataset_name, anndata, split, seed):\n",
    "    finest_level = hierarchy[-1]\n",
    "    cell_types = sorted(anndata.obs[finest_level].unique())\n",
    "\n",
    "    int_mapping = {groups: i for i, groups in enumerate(cell_types)}\n",
    "    with open(f\"../Data/{dataset_name}_int_mapping_flat.json\", \"w\") as file:\n",
    "        json.dump(int_mapping, file, indent=4)\n",
    "\n",
    "    anndata.obs[\"cell_integers\"] = anndata.obs[finest_level].map(int_mapping)\n",
    "\n",
    "    anndata_hvg = anndata[:, anndata.var['highly_variable']].copy()\n",
    "\n",
    "    if scipy.sparse.issparse(anndata_hvg.X):\n",
    "        X = anndata_hvg.X.toarray()\n",
    "    else:\n",
    "        X = anndata_hvg.X\n",
    "    \n",
    "    y = anndata_hvg.obs[\"cell_integers\"].values\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(X, y, test_size=split, random_state=seed, stratify=y)\n",
    "    weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "    train_features = np.array(train_features)\n",
    "    val_features = np.array(val_features)\n",
    "    train_labels = np.array(train_labels)\n",
    "    val_labels = np.array(val_labels)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    np.save(f'../Arrays/{dataset_name}_train_features_hvg_flat.npy', train_features)\n",
    "    np.save(f'../Arrays/{dataset_name}_val_features_hvg_flat.npy', val_features)\n",
    "    np.save(f'../Arrays/{dataset_name}_train_labels_hvg_flat.npy', train_labels)\n",
    "    np.save(f'../Arrays/{dataset_name}_val_labels_hvg_flat.npy', val_labels)\n",
    "    np.save(f'../Arrays/{dataset_name}_weights_hvg_flat.npy', weights)\n",
    "\n",
    "    anndata_hvg.write(f\"../Data/{dataset_name}_train_anndata_hvg_flat.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 6296\n",
    "split = 0.2\n",
    "# flat_classification(\"controls_final\", controls_final_anndata, split, seed)\n",
    "# flat_classification(\"granulomas_final\", granulomas_final_anndata, split, seed)\n",
    "flat_classification(\"sc78_final\", sc78_final_anndata, split, seed)\n",
    "flat_classification(\"sc92_final\", sc92_final_anndata, split, seed)\n",
    "flat_classification(\"sc93_final\", sc93_final_anndata, split, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793711e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name, model, index_to_child, children):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "        self.index_to_child = index_to_child\n",
    "        self.children = children\n",
    "\n",
    "    def predict(self, gene):\n",
    "        \n",
    "        if (not self.children): return self.name\n",
    "\n",
    "        if ((self.model == None) or (len(self.children) == 1)):\n",
    "            child_node = next(iter(self.children.values()))\n",
    "            return child_node.predict(gene)\n",
    "\n",
    "        logits = self.model.predict(np.array([gene]), verbose=0)\n",
    "        max_index = np.argmax(logits, axis=1)[0]\n",
    "        child_name = self.index_to_child[max_index]\n",
    "\n",
    "        return self.children[child_name].predict(gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c1d7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(dataset, hierarchy_dict):\n",
    "    def recurse(node_name, subtree):\n",
    "\n",
    "        name = create_name(node_name)\n",
    "        model_path = f\"../Models/{dataset}_hvg_{name}_jax_v1.keras\"\n",
    "        model = None\n",
    "        \n",
    "        if (os.path.exists(model_path)):\n",
    "            model = ks.models.load_model(model_path, custom_objects={'LeakyReLU': ks.layers.LeakyReLU}, compile=False)\n",
    "            \n",
    "        index_to_child = None\n",
    "        if subtree:\n",
    "            children = list(subtree)\n",
    "            int_mapping = f\"../Data/{dataset}_int_mapping_{name}.json\"\n",
    "\n",
    "            if (os.path.exists(int_mapping)):\n",
    "                with open(int_mapping) as file:\n",
    "                    child_to_index = json.load(file)\n",
    "\n",
    "                index_to_child = {int(value): key for key, value in child_to_index.items()}\n",
    "\n",
    "            else: index_to_child = {i: child for i, child in enumerate(children)}\n",
    "            child_nodes = {child: recurse(child, subtree[child]) for child in children}\n",
    "        \n",
    "        else: child_nodes = {}\n",
    "        return Node(node_name, model, index_to_child, child_nodes)\n",
    "    \n",
    "    return recurse(\"top_level\", hierarchy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d688d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_predictions(dataset, hierarchy_dict):\n",
    "    test_anndata = sc.read_h5ad(f\"../Data/{dataset}_test_anndata.h5ad\")\n",
    "    test_anndata_hvg = test_anndata[:, test_anndata.var['highly_variable'] ].copy()\n",
    "\n",
    "    root = create_tree(dataset, hierarchy_dict)\n",
    "\n",
    "    X = test_anndata_hvg.X\n",
    "\n",
    "    if scipy.sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "\n",
    "    predictions = []\n",
    "    for gene in X:\n",
    "        predictions.append(root.predict(gene))\n",
    "\n",
    "    test_anndata_hvg.obs['predicted_cell_type'] = predictions\n",
    "    true = test_anndata_hvg.obs['single_cell_types'].values\n",
    "\n",
    "    per_class_accuracy = (pd.Series(predictions == true).groupby(pd.Series(true)).mean().to_dict())\n",
    "    \n",
    "    overall = {\n",
    "        'accuracy': metrics.accuracy_score(true, predictions),\n",
    "        'balanced_accuracy': metrics.balanced_accuracy_score(true, predictions),\n",
    "        'precision': metrics.precision_score(true, predictions, average='macro', zero_division=0),\n",
    "        'recall': metrics.recall_score(true, predictions, average='macro', zero_division=0),\n",
    "        'f1_score': metrics.f1_score(true, predictions, average='macro', zero_division=0),\n",
    "        'AUPRC': 'N/A'\n",
    "    }\n",
    "\n",
    "    class_report = metrics.classification_report(true, predictions, digits=2, zero_division=0, output_dict=True)\n",
    "\n",
    "    class_report_df = (pd.DataFrame(class_report).transpose().rename_axis(\"label\").reset_index())\n",
    "\n",
    "    df = pd.DataFrame(X, columns=test_anndata_hvg.var.index.to_list())\n",
    "    df.insert(0, 'predicted_cell', predictions)\n",
    "    df.insert(0, 'true_cell', true)\n",
    "\n",
    "    xlsx_path = f\"Results/{dataset}_hierarchical_classification.xlsx\"\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
    "        pd.Series(overall, name=\"value\").to_frame().to_excel(writer, sheet_name=\"overall_metrics\")\n",
    "        pd.Series(per_class_accuracy, name=\"accuracy\").to_frame().to_excel(writer, sheet_name=\"per_cell_accuracy\")\n",
    "        class_report_df.to_excel(writer, sheet_name=\"classification_report\", index=False)\n",
    "        df.to_excel(writer, sheet_name=\"data\", index=False)\n",
    "\n",
    "    print(f\"Excel written to `{xlsx_path}`\")\n",
    "\n",
    "    test_anndata_hvg.write(f\"../Data/{dataset}_test_anndata_hierarchy_predictions.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9efd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hierarchy_dict(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f51f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls_final_hierarchy_dict = load_hierarchy_dict(\"../Data/controls_final_hierarchy_dict.json\")\n",
    "# granulomas_final_hierarchy_dict = load_hierarchy_dict(\"../Data/granulomas_final_hierarchy_dict.json\")\n",
    "sc78_final_hierarchy_dict = load_hierarchy_dict(\"../Data/sc78_final_hierarchy_dict.json\")\n",
    "sc92_final_hierarchy_dict = load_hierarchy_dict(\"../Data/sc92_final_hierarchy_dict.json\")\n",
    "sc93_final_hierarchy_dict = load_hierarchy_dict(\"../Data/sc93_final_hierarchy_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical_predictions(\"controls_final\", controls_final_hierarchy_dict)\n",
    "# hierarchical_predictions(\"granulomas_final\", granulomas_final_hierarchy_dict)\n",
    "hierarchical_predictions(\"sc78_final\", sc78_final_hierarchy_dict)\n",
    "hierarchical_predictions(\"sc92_final\", sc92_final_hierarchy_dict)\n",
    "hierarchical_predictions(\"sc93_final\", sc93_final_hierarchy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5961651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_prediction(dataset):\n",
    "    test_anndata = sc.read_h5ad(f\"../Data/{dataset}_test_anndata.h5ad\")\n",
    "    test_anndata_hvg = test_anndata[:, test_anndata.var['highly_variable'] ].copy()\n",
    "\n",
    "    model_path = f\"../Models/{dataset}_hvg_flat_jax_v1.keras\"\n",
    "    model = ks.models.load_model(model_path, custom_objects={'LeakyReLU': ks.layers.LeakyReLU}, compile=False)\n",
    "\n",
    "    X = test_anndata_hvg.X\n",
    "\n",
    "    if scipy.sparse.issparse(X):\n",
    "        X = X.toarray()\n",
    "\n",
    "    logits = model.predict(X)\n",
    "    max_indices = np.argmax(logits, axis=1)\n",
    "\n",
    "    path = f\"../Data/{dataset}_int_mapping_flat.json\"\n",
    "    with open(path) as file:\n",
    "        int_mapping = json.load(file)\n",
    "\n",
    "    inverse_dict = {i: j for j, i in int_mapping.items()}\n",
    "    predictions = [inverse_dict[i] for i in max_indices]\n",
    "\n",
    "    test_anndata_hvg.obs['predicted_cell_type'] = predictions\n",
    "    true = test_anndata_hvg.obs['single_cell_types'].values\n",
    "\n",
    "    per_class_accuracy = (pd.Series(predictions == true).groupby(pd.Series(true)).mean().to_dict())\n",
    "    \n",
    "    overall = {\n",
    "        'accuracy': metrics.accuracy_score(true, predictions),\n",
    "        'balanced_accuracy': metrics.balanced_accuracy_score(true, predictions),\n",
    "        'precision': metrics.precision_score(true, predictions, average='macro', zero_division=0),\n",
    "        'recall': metrics.recall_score(true, predictions, average='macro', zero_division=0),\n",
    "        'f1_score': metrics.f1_score(true, predictions, average='macro', zero_division=0),\n",
    "        'average_precision': metrics.average_precision_score(true, logits, average='macro')\n",
    "    }\n",
    "\n",
    "    class_report = metrics.classification_report(true, predictions, digits=2, zero_division=0, output_dict=True)\n",
    "\n",
    "    class_report_df = (pd.DataFrame(class_report).transpose().rename_axis(\"label\").reset_index())\n",
    "\n",
    "    df = pd.DataFrame(X, columns=test_anndata_hvg.var.index.to_list())\n",
    "    df.insert(0, 'predicted_cell', predictions)\n",
    "    df.insert(0, 'true_cell', true)\n",
    "\n",
    "    xlsx_path = f\"Results/{dataset}_flat_classification.xlsx\"\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
    "        pd.Series(overall, name=\"value\").to_frame().to_excel(writer, sheet_name=\"overall_metrics\")\n",
    "        pd.Series(per_class_accuracy, name=\"accuracy\").to_frame().to_excel(writer, sheet_name=\"per_cell_accuracy\")\n",
    "        class_report_df.to_excel(writer, sheet_name=\"classification_report\", index=False)\n",
    "        df.to_excel(writer, sheet_name=\"data\", index=False)\n",
    "\n",
    "    print(f\"Excel written to `{xlsx_path}`\")\n",
    "\n",
    "    test_anndata_hvg.write(f\"../Data/{dataset}_test_anndata_flat_predictions.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_prediction(\"controls_final\")\n",
    "# flat_prediction(\"granulomas_final\")\n",
    "flat_prediction(\"sc78_final\")\n",
    "flat_prediction(\"sc92_final\")\n",
    "flat_prediction(\"sc93_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e89ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e78ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1349d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a4da7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_csv from `anndata` is deprecated. Import anndata.io.read_csv instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_excel from `anndata` is deprecated. Import anndata.io.read_excel instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_hdf from `anndata` is deprecated. Import anndata.io.read_hdf instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_loom from `anndata` is deprecated. Import anndata.io.read_loom instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_mtx from `anndata` is deprecated. Import anndata.io.read_mtx instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\anndata\\utils.py:429: FutureWarning: Importing read_umi_tools from `anndata` is deprecated. Import anndata.io.read_umi_tools instead.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Source HVG] granulomas_final: 3475 genes\n",
      "\n",
      "=== FLAT transfer granulomas_final -> sc92_final ===\n",
      "[Cell-type filtering] Kept 6094 cells; dropped 291 (unknown to source).\n",
      "[Gene alignment] Target: 6094 cells; Source HVGs: 3475. Present in target: 3475. Padded zeros: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bence\\AppData\\Local\\Temp\\ipykernel_275532\\1415529970.py:147: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(pd.Series(true_labels))\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written to `Results\\granulomas_final_to_sc92_final_flat.xlsx`\n",
      "\n",
      "=== HIERARCHICAL transfer granulomas_final -> sc92_final ===\n",
      "[Cell-type filtering] Kept 6094 cells; dropped 291 (unknown to source).\n",
      "[Gene alignment] Target: 6094 cells; Source HVGs: 3475. Present in target: 3475. Padded zeros: 0.\n",
      "WARNING:tensorflow:5 out of the last 194 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028102E40720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 195 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028102E42AC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bence\\AppData\\Local\\Temp\\ipykernel_275532\\1415529970.py:147: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(pd.Series(true_labels))\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written to `Results\\granulomas_final_to_sc92_final_hierarchical.xlsx`\n",
      "\n",
      "=== FLAT transfer granulomas_final -> sc93_final ===\n",
      "[Cell-type filtering] Kept 4147 cells; dropped 0 (unknown to source).\n",
      "[Gene alignment] Target: 4147 cells; Source HVGs: 3475. Present in target: 3475. Padded zeros: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bence\\AppData\\Local\\Temp\\ipykernel_275532\\1415529970.py:147: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(pd.Series(true_labels))\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written to `Results\\granulomas_final_to_sc93_final_flat.xlsx`\n",
      "\n",
      "=== HIERARCHICAL transfer granulomas_final -> sc93_final ===\n",
      "[Cell-type filtering] Kept 4147 cells; dropped 0 (unknown to source).\n",
      "[Gene alignment] Target: 4147 cells; Source HVGs: 3475. Present in target: 3475. Padded zeros: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bence\\AppData\\Local\\Temp\\ipykernel_275532\\1415529970.py:147: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(pd.Series(true_labels))\n",
      "c:\\Users\\bence\\Projects\\BIO446\\McKinnon-Rosati-Laboratory\\env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel written to `Results\\granulomas_final_to_sc93_final_hierarchical.xlsx`\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ==== Cross-dataset label transfer: granulomas_30 -> granulomas_60/90 ====\n",
    "# Assumes your earlier imports are present (numpy, scanpy as sc, pandas, sklearn.metrics, keras as ks, etc.)\n",
    "# and that your preprocessing pipeline produced the usual files in ../Data and ../Models.\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import keras as ks\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "SOURCE = \"granulomas_final\"         # 30-day dataset (source models and feature order)\n",
    "TARGETS = [\"sc92_final\", \"sc93_final\"]  # 60-day, 90-day\n",
    "RESULTS_DIR = \"Results\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities\n",
    "# ------------------------------\n",
    "def _load_source_hvgs(source: str) -> pd.Index:\n",
    "    \"\"\"\n",
    "    Load the *exact* HVG order used to train the SOURCE models.\n",
    "    Prefer the flat training HVG file (created by flat_classification),\n",
    "    fall back to the base train anndata HVG if needed.\n",
    "    \"\"\"\n",
    "    flat_hvg_path = f\"../Data/{source}_train_anndata_hvg_flat.h5ad\"\n",
    "    if os.path.exists(flat_hvg_path):\n",
    "        ad = sc.read_h5ad(flat_hvg_path)\n",
    "        return ad.var_names.copy()\n",
    "    # # Fallback: use the train set and keep its HVG order\n",
    "    # base_train_path = f\"../Data/{source}_train_anndata.h5ad\"\n",
    "    # if not os.path.exists(base_train_path):\n",
    "    #     raise FileNotFoundError(\n",
    "    #         f\"Could not find {flat_hvg_path} nor {base_train_path}. \"\n",
    "    #         \"Please generate the source HVG training file first.\"\n",
    "    #     )\n",
    "    # ad = sc.read_h5ad(base_train_path)\n",
    "    # hvgs = ad.var.index[ad.var['highly_variable']].copy()\n",
    "    # return hvgs\n",
    "\n",
    "def _load_source_flat_model(source: str):\n",
    "    mpath = f\"../Models/{source}_hvg_flat_jax_v1.keras\"\n",
    "    if not os.path.exists(mpath):\n",
    "        return None\n",
    "    return ks.models.load_model(mpath, custom_objects={'LeakyReLU': ks.layers.LeakyReLU}, compile=False)\n",
    "\n",
    "def _load_source_hierarchy_dict(source: str):\n",
    "    jpath = f\"../Data/{source}_hierarchy_dict.json\"\n",
    "    if not os.path.exists(jpath):\n",
    "        return None\n",
    "    with open(jpath, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _load_source_flat_intmap(source: str):\n",
    "    jpath = f\"../Data/{source}_int_mapping_flat.json\"\n",
    "    if not os.path.exists(jpath):\n",
    "        raise FileNotFoundError(f\"Missing int mapping for source flat model: {jpath}\")\n",
    "    with open(jpath, \"r\") as f:\n",
    "        return json.load(f)  # {cell_type: int}\n",
    "\n",
    "def _align_to_source_genes(target_ad_full: sc.AnnData, source_hvgs: pd.Index) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a dense (float32) array of shape (n_obs_target, len(source_hvgs)),\n",
    "    where columns are in the *exact* order of source_hvgs.\n",
    "\n",
    "    We slice from the *full* target matrix (not just its HVGs), so if a source gene\n",
    "    exists in target.var_names (even if not HVG there), we use it. Otherwise we pad zeros.\n",
    "    \"\"\"\n",
    "    n = target_ad_full.n_obs\n",
    "    m = len(source_hvgs)\n",
    "    # Use normalized/log1p X (the same space your models trained on).\n",
    "    X_full = target_ad_full.X\n",
    "    # if not sp.issparse(X_full) and not isinstance(X_full, np.ndarray):\n",
    "    #     # AnnData may hold a backed array-like; force to CSR then work from there\n",
    "    #     X_full = sp.csr_matrix(X_full)\n",
    "\n",
    "    # Map target gene name -> column index in full matrix\n",
    "    tgt_varnames = target_ad_full.var_names\n",
    "    tgt_idx = {g: j for j, g in enumerate(tgt_varnames)}\n",
    "\n",
    "    present_positions_in_source = []\n",
    "    present_indices_in_target = []\n",
    "    for pos, g in enumerate(source_hvgs):\n",
    "        j = tgt_idx.get(g, None)\n",
    "        if j is not None:\n",
    "            present_positions_in_source.append(pos)\n",
    "            present_indices_in_target.append(j)\n",
    "\n",
    "    # Slice only present genes from sparse matrix -> dense\n",
    "    if len(present_indices_in_target) > 0:\n",
    "        X_present = X_full[:, present_indices_in_target]\n",
    "        if sp.issparse(X_present):\n",
    "            X_present = X_present.toarray()\n",
    "        else:\n",
    "            X_present = np.asarray(X_present)\n",
    "    else:\n",
    "        # None present? (extremely unlikely)\n",
    "        X_present = np.zeros((n, 0), dtype=np.float32)\n",
    "\n",
    "    # Allocate final and place present columns\n",
    "    X_aligned = np.zeros((n, m), dtype=np.float32)\n",
    "    if len(present_positions_in_source) > 0:\n",
    "        X_aligned[:, present_positions_in_source] = X_present.astype(np.float32, copy=False)\n",
    "\n",
    "    # Diagnostics\n",
    "    print(f\"[Gene alignment] Target: {target_ad_full.shape[0]} cells; \"\n",
    "          f\"Source HVGs: {m}. Present in target: {len(present_indices_in_target)}. \"\n",
    "          f\"Padded zeros: {m - len(present_indices_in_target)}.\")\n",
    "    return X_aligned\n",
    "\n",
    "def _filter_unknown_celltypes(target_ad: sc.AnnData, allowed_types: set) -> sc.AnnData:\n",
    "    \"\"\"\n",
    "    Keep only cells whose 'single_cell_types' are in allowed_types.\n",
    "    Returns a *copy* to avoid chained assignment issues.\n",
    "    \"\"\"\n",
    "    if 'single_cell_types' not in target_ad.obs.columns:\n",
    "        raise KeyError(\"target AnnData missing obs['single_cell_types']\")\n",
    "    mask = target_ad.obs['single_cell_types'].isin(allowed_types)\n",
    "    kept = int(mask.sum())\n",
    "    dropped = int((~mask).sum())\n",
    "    print(f\"[Cell-type filtering] Kept {kept} cells; dropped {dropped} (unknown to source).\")\n",
    "    return target_ad[mask].copy()\n",
    "\n",
    "def _save_results_xlsx(base_path: str, overall: dict, per_class_acc: dict, class_report: dict, X=None, true=None, pred=None, genes=None):\n",
    "    os.makedirs(os.path.dirname(base_path), exist_ok=True)\n",
    "    class_report_df = pd.DataFrame(class_report).transpose().rename_axis(\"label\").reset_index()\n",
    "    with pd.ExcelWriter(base_path, engine=\"openpyxl\") as writer:\n",
    "        pd.Series(overall, name=\"value\").to_frame().to_excel(writer, sheet_name=\"overall_metrics\")\n",
    "        pd.Series(per_class_acc, name=\"accuracy\").to_frame().to_excel(writer, sheet_name=\"per_cell_accuracy\")\n",
    "        class_report_df.to_excel(writer, sheet_name=\"classification_report\", index=False)\n",
    "        if X is not None and true is not None and pred is not None:\n",
    "            df = pd.DataFrame(X, columns=list(genes) if genes is not None else None)\n",
    "            df.insert(0, \"predicted_cell\", pred)\n",
    "            df.insert(0, \"true_cell\", true)\n",
    "            df.to_excel(writer, sheet_name=\"data\", index=False)\n",
    "    print(f\"Excel written to `{base_path}`\")\n",
    "\n",
    "def _compute_metrics(true_labels: np.ndarray, pred_labels: np.ndarray):\n",
    "    per_class_accuracy = (pd.Series(pred_labels == true_labels)\n",
    "                            .groupby(pd.Series(true_labels))\n",
    "                            .mean().to_dict())\n",
    "    overall = {\n",
    "        'accuracy': metrics.accuracy_score(true_labels, pred_labels),\n",
    "        'balanced_accuracy': metrics.balanced_accuracy_score(true_labels, pred_labels),\n",
    "        'precision': metrics.precision_score(true_labels, pred_labels, average='macro', zero_division=0),\n",
    "        'recall': metrics.recall_score(true_labels, pred_labels, average='macro', zero_division=0),\n",
    "        'f1_score': metrics.f1_score(true_labels, pred_labels, average='macro', zero_division=0),\n",
    "        'AUPRC': 'N/A'\n",
    "    }\n",
    "    class_report = metrics.classification_report(true_labels, pred_labels, digits=2, zero_division=0, output_dict=True)\n",
    "    return overall, per_class_accuracy, class_report\n",
    "\n",
    "# ------------------------------\n",
    "# Hierarchical prediction support (reuse your Node/create_tree with source dataset)\n",
    "# ------------------------------\n",
    "# class Node:\n",
    "#     def __init__(self, name, model, index_to_child, children):\n",
    "#         self.name = name\n",
    "#         self.model = model\n",
    "#         self.index_to_child = index_to_child\n",
    "#         self.children = children\n",
    "\n",
    "#     def predict_one(self, gene_vec: np.ndarray):\n",
    "#         if not self.children:\n",
    "#             return self.name\n",
    "#         if (self.model is None) or (len(self.children) == 1):\n",
    "#             child_node = next(iter(self.children.values()))\n",
    "#             return child_node.predict_one(gene_vec)\n",
    "#         logits = self.model.predict(np.array([gene_vec], dtype=np.float32), verbose=0)\n",
    "#         max_index = int(np.argmax(logits, axis=1)[0])\n",
    "#         child_name = self.index_to_child[max_index]\n",
    "#         return self.children[child_name].predict_one(gene_vec)\n",
    "\n",
    "# def _create_tree_for_source(source: str, hierarchy_dict: dict) -> Node:\n",
    "#     def recurse(node_name, subtree):\n",
    "#         name = re_sub_safe(node_name)\n",
    "#         model_path = f\"../Models/{source}_hvg_{name}_jax_v1.keras\"\n",
    "#         model = None\n",
    "#         if os.path.exists(model_path):\n",
    "#             model = ks.models.load_model(model_path, custom_objects={'LeakyReLU': ks.layers.LeakyReLU}, compile=False)\n",
    "\n",
    "#         if subtree:\n",
    "#             children = list(subtree)\n",
    "#             int_map_path = f\"../Data/{source}_int_mapping_{name}.json\"\n",
    "#             if os.path.exists(int_map_path):\n",
    "#                 with open(int_map_path) as f:\n",
    "#                     child_to_index = json.load(f)  # {child_name: int}\n",
    "#                 index_to_child = {int(v): k for k, v in child_to_index.items()}\n",
    "#             else:\n",
    "#                 index_to_child = {i: child for i, child in enumerate(children)}\n",
    "#             child_nodes = {child: recurse(child, subtree[child]) for child in children}\n",
    "#         else:\n",
    "#             index_to_child = None\n",
    "#             child_nodes = {}\n",
    "#         return Node(node_name, model, index_to_child, child_nodes)\n",
    "#     # helper: mirror your create_name\n",
    "#     import re as _re\n",
    "#     def re_sub_safe(s): return _re.sub(r\"[^A-Za-z0-9]+\", \"_\", s).strip(\"_\").lower()\n",
    "#     return recurse(\"top_level\", hierarchy_dict)\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-dataset transfer: FLAT\n",
    "# ------------------------------\n",
    "def cross_dataset_flat_transfer(source: str, target: str):\n",
    "    print(f\"\\n=== FLAT transfer {source} -> {target} ===\")\n",
    "\n",
    "    # Load source artifacts\n",
    "    int_map = _load_source_flat_intmap(source)                # {cell_type: int}\n",
    "    allowed_types = set(int_map.keys())\n",
    "    inverse_int = {v: k for k, v in int_map.items()}\n",
    "\n",
    "    source_hvgs = _load_source_hvgs(source)\n",
    "    model = _load_source_flat_model(source)\n",
    "    if model is None:\n",
    "        print(f\"[SKIP] Flat model not found for source: ../Models/{source}_hvg_flat_jax_v1.keras\")\n",
    "        return\n",
    "\n",
    "    # # Load target full test set (all genes), then filter and align\n",
    "    tgt_test_path = f\"../Data/{target}_test_anndata.h5ad\"\n",
    "    if not os.path.exists(tgt_test_path):\n",
    "        raise FileNotFoundError(f\"Missing target test AnnData: {tgt_test_path}\")\n",
    "    ad_tgt = sc.read_h5ad(tgt_test_path)\n",
    "\n",
    "    # # Filter out unknown cell types\n",
    "    ad_tgt = _filter_unknown_celltypes(ad_tgt, allowed_types)\n",
    "    if ad_tgt.n_obs == 0:\n",
    "        print(\"[WARN] No cells left after filtering; nothing to evaluate.\")\n",
    "        return\n",
    "\n",
    "    # # Align to source HVGs (use full var_names, not only HVGs)\n",
    "    X_aligned = _align_to_source_genes(ad_tgt, source_hvgs)  # dense float32\n",
    "\n",
    "    # # Predict\n",
    "    logits = model.predict(X_aligned, verbose=0)\n",
    "    max_idx = np.argmax(logits, axis=1)\n",
    "    preds = [inverse_int[int(i)] for i in max_idx]\n",
    "    true = ad_tgt.obs['single_cell_types'].values\n",
    "\n",
    "    # # Metrics + save\n",
    "    overall, per_class_acc, class_report = _compute_metrics(true, np.array(preds, dtype=object))\n",
    "    base = os.path.join(RESULTS_DIR, f\"{source}_to_{target}_flat.xlsx\")\n",
    "    _save_results_xlsx(base, overall, per_class_acc, class_report, X=X_aligned, true=true, pred=preds, genes=source_hvgs)\n",
    "\n",
    "    # # Save a copy of the target AnnData with predictions for downstream exploration\n",
    "    # ad_out = ad_tgt.copy()\n",
    "    # ad_out.X = X_aligned  # aligned feature space\n",
    "    # ad_out.var = pd.DataFrame(index=source_hvgs)  # reflect aligned genes\n",
    "    # ad_out.obs['predicted_cell_type'] = preds\n",
    "    # out_h5ad = f\"../Data/{target}_test_anndata_flat_from_{source}_predictions.h5ad\"\n",
    "    # ad_out.write(out_h5ad)\n",
    "    # print(f\"H5AD with predictions written to `{out_h5ad}`\")\n",
    "\n",
    "# ------------------------------\n",
    "# Cross-dataset transfer: HIERARCHICAL\n",
    "# ------------------------------\n",
    "def cross_dataset_hier_transfer(source: str, target: str):\n",
    "    print(f\"\\n=== HIERARCHICAL transfer {source} -> {target} ===\")\n",
    "\n",
    "    # Load hierarchy dict and build tree\n",
    "    hierarchy = _load_source_hierarchy_dict(source)\n",
    "    if hierarchy is None:\n",
    "        print(f\"[SKIP] Hierarchy dict not found for source: ../Data/{source}_hierarchy_dict.json\")\n",
    "        return\n",
    "    root = create_tree(source, hierarchy)\n",
    "\n",
    "    # Allowed labels are the leaves of the source hierarchy (finest level)\n",
    "    allowed_types = set(get_leaves(hierarchy))\n",
    "\n",
    "    # Load source HVGs for alignment\n",
    "    source_hvgs = _load_source_hvgs(source)\n",
    "\n",
    "    # Load target test (full), filter, align\n",
    "    tgt_test_path = f\"../Data/{target}_test_anndata.h5ad\"\n",
    "    if not os.path.exists(tgt_test_path):\n",
    "        raise FileNotFoundError(f\"Missing target test AnnData: {tgt_test_path}\")\n",
    "    ad_tgt = sc.read_h5ad(tgt_test_path)\n",
    "    ad_tgt = _filter_unknown_celltypes(ad_tgt, allowed_types)\n",
    "    if ad_tgt.n_obs == 0:\n",
    "        print(\"[WARN] No cells left after filtering; nothing to evaluate.\")\n",
    "        return\n",
    "\n",
    "    X_aligned = _align_to_source_genes(ad_tgt, source_hvgs)\n",
    "\n",
    "    # Predict by traversing the tree\n",
    "    preds = []\n",
    "    for i in range(X_aligned.shape[0]):\n",
    "        preds.append(root.predict(X_aligned[i, :]))\n",
    "    true = ad_tgt.obs['single_cell_types'].values\n",
    "\n",
    "    # Metrics + save\n",
    "    overall, per_class_acc, class_report = _compute_metrics(true, np.array(preds, dtype=object))\n",
    "    base = os.path.join(RESULTS_DIR, f\"{source}_to_{target}_hierarchical.xlsx\")\n",
    "    _save_results_xlsx(base, overall, per_class_acc, class_report,X=X_aligned, true=true, pred=preds, genes=source_hvgs)\n",
    "\n",
    "    # Save H5AD with predictions\n",
    "    # ad_out = ad_tgt.copy()\n",
    "    # ad_out.X = X_aligned\n",
    "    # ad_out.var = pd.DataFrame(index=source_hvgs)\n",
    "    # ad_out.obs['predicted_cell_type'] = preds\n",
    "    # out_h5ad = f\"../Data/{target}_test_anndata_hier_from_{source}_predictions.h5ad\"\n",
    "    # ad_out.write(out_h5ad)\n",
    "    # print(f\"H5AD with predictions written to `{out_h5ad}`\")\n",
    "\n",
    "# ------------------------------\n",
    "# Run transfers for requested targets\n",
    "# ------------------------------\n",
    "source_hvgs_preview = _load_source_hvgs(SOURCE)\n",
    "print(f\"[Source HVG] {SOURCE}: {len(source_hvgs_preview)} genes\")\n",
    "\n",
    "for tgt in TARGETS:\n",
    "    # Flat transfer\n",
    "    cross_dataset_flat_transfer(SOURCE, tgt)\n",
    "    # Hierarchical transfer (if artifacts exist)\n",
    "    cross_dataset_hier_transfer(SOURCE, tgt)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
